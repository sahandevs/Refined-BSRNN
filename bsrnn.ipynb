{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "741a4d6f-e587-4f57-8a1d-1c54412d69d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "import IPython.display as idp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ee4ca1-ac3e-40d7-845a-23f6c4f625ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e425c21-aa1a-419c-904e-c68e9a683582",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1) Input and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a16cbc51-70f7-41c4-be4c-cb6a7c5debd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load audio\n",
    "\n",
    "sample_rate = 44100 # \n",
    "\n",
    "def show_idp_audio(waveform):\n",
    "    n = 14\n",
    "    return idp.display(idp.Audio(waveform[(3 * n) * sample_rate:(3 * (n + 1)) * sample_rate], rate=sample_rate))\n",
    "\n",
    "def load_audio(path, visualize=False):\n",
    "    waveform, sr = torchaudio.load(path)\n",
    "    # Convert everthing to mono channel for simplicity\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0)\n",
    "        # waveform is now a vector \n",
    "    # Resample everything to 44.1khz for simplicity\n",
    "    resampler = T.Resample(sr, sample_rate, dtype=waveform.dtype)\n",
    "    waveform = resampler(waveform)\n",
    "    \n",
    "    if visualize:\n",
    "        # samplerate = 1/t\n",
    "        # display the first 3 seconds\n",
    "        show_idp_audio(waveform)\n",
    "    \n",
    "    return waveform\n",
    "\n",
    "if visualize:\n",
    "    sample_waveform = load_audio(\"mixture.wav\", visualize=True)\n",
    "\n",
    "    sample_waveform.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45db49d1-7007-4b1f-a9d0-e4a68dfb773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_normalize(waveform, target_rms):\n",
    "    current_rms = torch.sqrt(torch.mean(waveform**2))\n",
    "    gain_factor = target_rms / (current_rms + 1e-10)\n",
    "    normalized_waveform = waveform * gain_factor\n",
    "    return normalized_waveform, gain_factor\n",
    "\n",
    "def rms_denormalize(normalized_waveform, gain_factor):\n",
    "    inverse_gain = 1 / gain_factor\n",
    "    reversed_waveform = normalized_waveform * inverse_gain\n",
    "    return reversed_waveform\n",
    "\n",
    "def peak_normalize(waveform, target_peak):\n",
    "    peak_value = torch.max(torch.abs(waveform))\n",
    "    peak_gain_factor = target_peak / (peak_value + 1e-10)\n",
    "    normalized_waveform = waveform * peak_gain_factor\n",
    "    return normalized_waveform, peak_gain_factor\n",
    "\n",
    "def peak_denormalize(normalized_waveform, peak_gain_factor):\n",
    "    inverse_peak_gain = 1 / peak_gain_factor\n",
    "    reversed_waveform = normalized_waveform * inverse_peak_gain\n",
    "    return reversed_waveform\n",
    "\n",
    "def inspect_waveform(waveform):\n",
    "    transform = T.Loudness(sample_rate)\n",
    "    return f\"LKFS:{transform(waveform.unsqueeze(0))} max: {waveform.max()} min: {waveform.min()} avg: {waveform.mean()}\"\n",
    "\n",
    "def normalize_waveform(waveform, visualize=False):\n",
    "    \"\"\" rms -> peak \"\"\"\n",
    "    # target rms can be anything. the important part here\n",
    "    # is to be constant for all kind of songs\n",
    "\n",
    "    if visualize:\n",
    "        print(\"original: \" + inspect_waveform(waveform))\n",
    "        show_idp_audio(waveform)\n",
    "\n",
    "    normalized_waveform, gain_factor = rms_normalize(waveform, target_rms=0.1)\n",
    "    \n",
    "    if visualize:\n",
    "        print(\"rms_normalize: \" + inspect_waveform(normalized_waveform))\n",
    "        show_idp_audio(normalized_waveform)\n",
    "\n",
    "    # setting target peak to 1.0 forces the values between -1.0 < y < 1.0\n",
    "    normalized_waveform, peak_gain_factor = peak_normalize(normalized_waveform, target_peak=0.1)\n",
    "    \n",
    "    if visualize:\n",
    "        print(\"peak_normalize: \" + inspect_waveform(normalized_waveform))\n",
    "        show_idp_audio(normalized_waveform)\n",
    "    \n",
    "    return normalized_waveform, gain_factor, peak_gain_factor\n",
    "\n",
    "def de_normalize_waveform(waveform, gain_factor, peak_gain_factor, visualize=False):\n",
    "    if visualize:\n",
    "        print(\"de_normalize_waveform: \" + inspect_waveform(waveform))\n",
    "        show_idp_audio(waveform)\n",
    "    \n",
    "    waveform = peak_denormalize(waveform, peak_gain_factor)\n",
    "    \n",
    "    if visualize:\n",
    "        print(\"peak_denormalize: \" + inspect_waveform(waveform))\n",
    "        show_idp_audio(waveform)\n",
    "    waveform = rms_denormalize(waveform, gain_factor)\n",
    "    \n",
    "    if visualize:\n",
    "        print(\"rms_denormalize: \" + inspect_waveform(waveform))\n",
    "        show_idp_audio(waveform)\n",
    "    \n",
    "    return waveform\n",
    "\n",
    "if visualize:\n",
    "    normal_waveform, gain_factor, peak_gain_factor = normalize_waveform(sample_waveform, visualize=True)\n",
    "    _ = de_normalize_waveform(normal_waveform, gain_factor, peak_gain_factor, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7787ec7-063b-47f3-9549-578fc962cd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size_in_seconds = 1\n",
    "chunk_size = chunk_size_in_seconds * sample_rate\n",
    "\n",
    "def split(waveform, visualize=False):\n",
    "    # we have a vector by length n and we want to split it to even chunks by length of\n",
    "    # chunk_size\n",
    "    padding_length = (chunk_size - waveform.shape[0] % chunk_size) % chunk_size\n",
    "    waveform = nn.functional.pad(waveform, (0, padding_length), 'constant', 0)\n",
    "    # -1 means automatically infer based on other dims\n",
    "    chunked_waveform = waveform.view(-1, chunk_size)\n",
    "    \n",
    "    if visualize:\n",
    "        fig = plt.figure(constrained_layout=True, figsize=(16, 4))\n",
    "        subfigs = fig.subfigures(2, 1).flat\n",
    "        \n",
    "        # first 3 chunk_size of waveform\n",
    "        w = waveform[:3 * chunk_size].detach().numpy()\n",
    "        ylim = [w.max() * 1.1, w.min() * 1.1]\n",
    "        def time_axis(start, duration):\n",
    "            return torch.arange(start * sample_rate, (duration + start) * sample_rate) / sample_rate\n",
    "        axes = subfigs[0].subplots(1, 1)\n",
    "        axes.plot(time_axis(0, 3), w, linewidth=0.3)\n",
    "        axes.set_xlabel(\"time [s] for first 3 seconds\")\n",
    "        axes.set_ylim(ylim)\n",
    "        \n",
    "        # first 4 chunks + last chunk\n",
    "        axes = subfigs[1].subplots(1, 5)\n",
    "        for i, chunk in enumerate([0, 1, 3, 4, chunked_waveform.shape[0] - 1]): \n",
    "            axes[i].plot(time_axis(0, chunk_size_in_seconds), chunked_waveform[chunk], linewidth=0.3)\n",
    "            axes[i].set_title(f\"chunk {chunk}\")\n",
    "            axes[i].set_ylim(ylim)\n",
    "        \n",
    "    return chunked_waveform, padding_length\n",
    "\n",
    "def merge(chunks, padding_length):\n",
    "    merged_waveform = torch.cat([torch.flatten(x) for x in chunks])\n",
    "    return merged_waveform[:-padding_length]\n",
    "\n",
    "if visualize:\n",
    "    sample_waveform_chunks, padding_length = split(normal_waveform, visualize=True)\n",
    "\n",
    "    assert sample_waveform_chunks.shape[1] == chunk_size\n",
    "\n",
    "    sample_merged = merge(sample_waveform_chunks, padding_length)\n",
    "\n",
    "    assert sample_merged.shape == normal_waveform.shape\n",
    "    assert torch.all(sample_merged == normal_waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d3f9132-958f-4704-8ad9-3a22c4138b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 2048\n",
    "win_length = n_fft\n",
    "hop_length = win_length // 4\n",
    "\n",
    "\n",
    "def visualize_spectogram(chunk, chunk_stft, title='Spectogram'):\n",
    "    import librosa\n",
    "    fig, axis = plt.subplots(2, 1, figsize=(16, 5))\n",
    "    noverlap = win_length - hop_length\n",
    "    axis[0].imshow(librosa.power_to_db(chunk_stft.abs().detach().numpy() ** 2), origin=\"lower\", aspect=\"auto\", interpolation=\"nearest\")\n",
    "    axis[0].set_yscale(\"symlog\")\n",
    "    axis[0].set_title(title)\n",
    "    if chunk is not None:\n",
    "        axis[1].plot(chunk, linewidth=0.5)\n",
    "        axis[1].grid(True)\n",
    "        axis[1].set_xlim([0, len(chunk)])\n",
    "\n",
    "def to_spectogram():\n",
    "    transform_spectogram = T.Spectrogram(n_fft=n_fft, win_length=win_length, hop_length=hop_length,\\\n",
    "                                         window_fn=torch.hamming_window, power=None)\n",
    "    \n",
    "    def inner(chunk, visualize=False, title=''):\n",
    "        chunk_stft = transform_spectogram(chunk)\n",
    "        if visualize:\n",
    "            visualize_spectogram(chunk, chunk_stft, title)\n",
    "\n",
    "        return chunk_stft\n",
    "    return inner\n",
    "\n",
    "if visualize:\n",
    "    print(sample_waveform_chunks[1].shape)\n",
    "    chunk_stft = to_spectogram()(sample_waveform_chunks[1], visualize=True)\n",
    "    chunk_stft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df5e3f90-057c-48b1-8340-6b158236836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vis:\n",
    "    def visualize(self, input):\n",
    "        from torchview import draw_graph\n",
    "        y = self(input)\n",
    "        x = draw_graph(self, input_data=input, device='meta', roll=True)\n",
    "        print(f\"--{input.shape}-->f(x)--{y.shape}-->\")\n",
    "        file = x.visual_graph.render(self._get_name())\n",
    "        display(idp.FileLink(\"./\" + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8adf8d8e-eda8-4354-9256-df3786f80e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numbers are exctracted from the paper\n",
    "splits_v7 = [\n",
    "   # below 1kh, bandwidth 100hz\n",
    "   (1000, 100),\n",
    "   # above 1kh and below 4khz, bandwidth 250hz\n",
    "   (4000, 250),\n",
    "   (8000, 500),\n",
    "   (16000, 1000),\n",
    "   (20000, 2000),\n",
    "]\n",
    "\n",
    "temporal_dim = int(np.ceil(chunk_size / T.Spectrogram(n_fft=n_fft, win_length=win_length, hop_length=hop_length).hop_length))\n",
    "feature_dim = 128 // 4\n",
    "\n",
    "# Module 1\n",
    "class BandSplit(nn.Module, Vis):\n",
    "    \n",
    "    def __init__(self, splits=splits_v7, fully_connected_out=feature_dim):\n",
    "        super(BandSplit, self).__init__()\n",
    "        \n",
    "        \n",
    "        #### Make splits\n",
    "        # convert fft to freq\n",
    "        freqs = sample_rate * torch.fft.fftfreq(n_fft)[:n_fft // 2 + 1]\n",
    "        freqs[-1] = sample_rate // 2\n",
    "        indices = []\n",
    "        start_freq, start_index = 0, 0\n",
    "        for end_freq, step in splits:\n",
    "            bands = torch.arange(start_freq + step, end_freq + step, step)\n",
    "            start_freq = end_freq\n",
    "            for band in bands:\n",
    "                end_index = freqs[freqs < band].shape[0]\n",
    "                indices.append((start_index, end_index))\n",
    "                start_index = end_index\n",
    "        indices.append((start_index, freqs.shape[0]))\n",
    "        self.band_indices = indices\n",
    "        self.fully_connected_out = fully_connected_out\n",
    "        \n",
    "        self.layer_norms = nn.ModuleList([\n",
    "            # * 2 is for added dim of view_as_real\n",
    "            nn.LayerNorm([(band_end - band_start) * 2, temporal_dim])\n",
    "            for band_start, band_end in self.band_indices\n",
    "        ])\n",
    "        \n",
    "        self.layer_fcs =  nn.ModuleList([\n",
    "            # * 2 is for added dim of view_as_real\n",
    "            nn.Linear((band_end - band_start) * 2, fully_connected_out)\n",
    "            for band_start, band_end in self.band_indices\n",
    "        ])\n",
    "\n",
    "    def forward(self, chunk_ftt):\n",
    "        batch_size = chunk_ftt.size(0)\n",
    "        stack = []\n",
    "        # TODO: can i vectorize this loop?\n",
    "        for i, (band_start, band_end) in enumerate(self.band_indices):\n",
    "            band = chunk_ftt[:, band_start:band_end, :]\n",
    "            # band is shape of (B, F, T)\n",
    "            band = torch.view_as_real(band) # (B, F, T, 2)\n",
    "            # convert to (B, 2, F, T) to be able to feed it to the norm\n",
    "            band = band.permute(0, 3, 1, 2)\n",
    "            \n",
    "            # norm is (..., F, T) and fc is (Fxfully_connected_out)\n",
    "            # we should make norm (..., T, F) in order to feed it to the fc\n",
    "            band = band.reshape(batch_size, -1, band.size(-1)) # -1 = T\n",
    "            norm = self.layer_norms[i](band)\n",
    "            \n",
    "            norm = norm.transpose(-1, -2).contiguous()\n",
    "            fc_y = self.layer_fcs[i](norm)\n",
    "            \n",
    "            stack.append(fc_y)\n",
    "        return torch.stack(stack, dim=1)\n",
    "\n",
    "if visualize:\n",
    "    bandsplit_layer = BandSplit()\n",
    "    bandsplit_y = bandsplit_layer(chunk_stft.unsqueeze(0))\n",
    "    bandsplit_layer.visualize(chunk_stft.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca7dfeea-f1c3-41a7-a4ff-06b0f0e39e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 2\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_dim_size = input_dim_size\n",
    "        # paper specified group norm\n",
    "        self.norm = nn.ModuleList([nn.GroupNorm(self.input_dim_size, self.input_dim_size) for _ in range(2)])\n",
    "        self.blstm = nn.ModuleList([nn.LSTM(self.input_dim_size, self.input_dim_size, bidirectional=True, batch_first=True) for _ in range(2)])\n",
    "        self.fc = nn.ModuleList([nn.Linear(self.input_dim_size * 2, self.input_dim_size) for _ in range(2)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # input is b, bands(K), temporal_dim(t), input_dim_size\n",
    "        \n",
    "        \n",
    "        # First loops converts the shape to [B, T, K, N]\n",
    "        # and the second loop converts it back to [B, K, T, N]\n",
    "        for i in range(2):\n",
    "            B, K, T, N = x.shape\n",
    "            out = x.view(B * K, T, N)\n",
    "            out = self.norm[i](out.transpose(-1, -2)).transpose(-1, -2)\n",
    "            out = self.blstm[i](out)[0]\n",
    "            out = self.fc[i](out)\n",
    "            x = out.view(B, K, T, N) + x\n",
    "            x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        return x\n",
    "\n",
    "num_blstm_layers=24 // 2\n",
    "\n",
    "class BandSequence(nn.Module, Vis):\n",
    "    \n",
    "    def __init__(self, input_dim_size, num_layers=num_blstm_layers):\n",
    "        super(BandSequence, self).__init__()\n",
    "        self.rnns = nn.Sequential(*[RNN(input_dim_size=input_dim_size) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # (bands, temporal_dim, fc_out)\n",
    "        return self.rnns(x)\n",
    "\n",
    "if visualize:\n",
    "    bandsequence_layer = BandSequence(input_dim_size=bandsplit_layer.fully_connected_out)\n",
    "    bandsequence_y = bandsequence_layer(bandsplit_y)\n",
    "    bandsequence_layer.visualize(bandsplit_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9fe8f3d-b9ff-4e24-b532-bbbdfd45b7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class MaskEstimation(nn.Module, Vis):\n",
    "    def __init__(self, band_indices, fully_connected_out):\n",
    "        super(MaskEstimation, self).__init__()\n",
    "        \n",
    "        max_indice_diff = max([e - s for s, e in band_indices])\n",
    "        num_hiddens = lambda e, s: 3 * (max_indice_diff - (e - s) + 1)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.LayerNorm([temporal_dim, fully_connected_out]),\n",
    "                nn.Linear(fully_connected_out, num_hiddens(e, s)),\n",
    "                nn.Tanh(),\n",
    "                # double the output dim to use in GLU\n",
    "                # the extra *2 is for returning as complex\n",
    "                nn.Linear(num_hiddens(e, s), (e - s) * 2 * 2),\n",
    "                nn.GLU()\n",
    "            )\n",
    "            for s, e in band_indices\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # (b, k, temporal_dim, fc_out)\n",
    "        parts = []\n",
    "        for i in range(x.shape[1]):\n",
    "            y = self.layers[i](x[:, i]).contiguous()\n",
    "            B, T, F = y.shape\n",
    "            y = y.permute(0, 2, 1).contiguous() # B F T\n",
    "            # basically halve the freq dim and use it for phasee\n",
    "            y = y.view(B, 2, F // 2, T) # (B, 2, F, T)\n",
    "            y = y.permute(0, 2, 3, 1) # (B, F, T, 2)\n",
    "            y = torch.view_as_complex(y.contiguous())\n",
    "            \n",
    "            parts.append(y)\n",
    "        \n",
    "        # (b, f, t)\n",
    "        return torch.cat(parts, dim=-2)\n",
    "\n",
    "    \n",
    "if visualize:   \n",
    "    mask_layer = MaskEstimation(band_indices=bandsplit_layer.band_indices, fully_connected_out=bandsplit_layer.fully_connected_out)\n",
    "    mask_y = mask_layer(bandsequence_y)\n",
    "    mask_layer.visualize(bandsequence_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "848d0886-e194-4a72-8a81-b27554be18a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSRNN(nn.Module, Vis):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BSRNN, self).__init__()\n",
    "        \n",
    "        self.split = BandSplit()\n",
    "        self.sequence = BandSequence(input_dim_size=self.split.fully_connected_out)\n",
    "        self.mask = MaskEstimation(band_indices=self.split.band_indices, fully_connected_out=self.split.fully_connected_out)\n",
    "\n",
    "    def forward(self, chunk_fft):\n",
    "        \n",
    "        mean = chunk_fft.mean(dim=(1, 2), keepdim=True)\n",
    "        std = chunk_fft.std(dim=(1, 2), keepdim=True)\n",
    "        chunk_fft = (chunk_fft - mean) / (std + 1e-5)\n",
    "        \n",
    "        y = self.split(chunk_fft)\n",
    "        y = self.sequence(y)\n",
    "        mask = self.mask(y)\n",
    "        \n",
    "        mask = mask * std + mean\n",
    "\n",
    "        return mask\n",
    "\n",
    "if visualize:\n",
    "    bsrnn = BSRNN().cuda()\n",
    "    bsrnn_y = bsrnn(chunk_stft.unsqueeze(0).cuda())\n",
    "    bsrnn.visualize(chunk_stft.unsqueeze(0).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ac4f830-f48c-4900-903b-593ab23c4bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def from_spectogram():\n",
    "    # We are using GriffinLim to ensure the output size\n",
    "    transform_inv_spectogram = T.InverseSpectrogram(n_fft=n_fft, win_length=win_length, hop_length=hop_length,\\\n",
    "                                                    window_fn=torch.hamming_window)\n",
    "\n",
    "    def inner(chunk_stft, visualize=False):\n",
    "        chunk = transform_inv_spectogram(chunk_stft)\n",
    "    \n",
    "        if visualize:\n",
    "            visualize_spectogram(chunk.detach().numpy(), chunk_stft)\n",
    "\n",
    "        return chunk\n",
    "    return inner\n",
    "\n",
    "if visualize:\n",
    "    print(bsrnn_y.shape)\n",
    "    masked_complex = chunk_stft * bsrnn_y[0].cpu()\n",
    "\n",
    "    chunk_y = from_spectogram()(masked_complex, visualize=True)\n",
    "    chunk_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf3b1032-7bc6-40f9-9745-854c3be034b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize:\n",
    "    chunk_x = de_normalize_waveform(sample_waveform_chunks[1], gain_factor, peak_gain_factor)\n",
    "    chunk_y = de_normalize_waveform(chunk_y, gain_factor, peak_gain_factor)\n",
    "\n",
    "    print(\"In:\")\n",
    "    show_idp_audio(chunk_x)\n",
    "    print(\"Out:\")\n",
    "    show_idp_audio(chunk_y.detach().numpy())\n",
    "    print(chunk_y.shape)\n",
    "    chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05ed9801-0fde-4110-a40f-95b20d787857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b72ccde-384a-40da-b144-1206e527d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put Everything Together! audio in, audio out!\n",
    "\n",
    "\n",
    "class MSSBandSplitRNN(nn.Module, Vis):\n",
    "    def __init__(self):\n",
    "        super(MSSBandSplitRNN, self).__init__()\n",
    "        self.to_spectogram = to_spectogram()\n",
    "        self.from_spectogram = from_spectogram()\n",
    "        self.bsrnn = BSRNN()\n",
    "        \n",
    "    def forward(self, waveform):\n",
    "        \"\"\" Waveform in -> Waveform out :) \"\"\"\n",
    "        \n",
    "        # 1) normalize\n",
    "        # 2) split\n",
    "        # 3) feed to bsrnn\n",
    "        # 4) convert spectogram to audio\n",
    "        # 5) merge all splits\n",
    "        # 6) de-normalize\n",
    "        \n",
    "        normal_waveform, gain_factor, peak_gain_factor = normalize_waveform(waveform)\n",
    "        splits, padding_length = split(normal_waveform)\n",
    "        masked_splits = [() for _ in range(len(splits))]\n",
    "        for i, x_split in enumerate(splits):\n",
    "            split_stft = self.to_spectogram(x_split)\n",
    "            mask = self.bsrnn(split_stft.unsqueeze(0))[0]\n",
    "            \n",
    "            masked_complex = mask\n",
    "            \n",
    "            wave = self.from_spectogram(masked_complex)\n",
    "            masked_splits[i] = wave\n",
    "        \n",
    "        masked_waveform = merge(masked_splits, padding_length)\n",
    "        y = de_normalize_waveform(masked_waveform, gain_factor, peak_gain_factor)\n",
    "        return y\n",
    "\n",
    "if visualize:\n",
    "    torch.set_default_device('cuda')\n",
    "    with torch.no_grad():\n",
    "        model = MSSBandSplitRNN()\n",
    "        y = model(sample_waveform.cuda())\n",
    "\n",
    "\n",
    "    print(\"In:\")\n",
    "    show_idp_audio(sample_waveform)\n",
    "    print(\"Out:\")\n",
    "    show_idp_audio(y.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b004c354-c93f-4d13-a326-3197b0b8a38f",
   "metadata": {},
   "source": [
    "# 2) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75190bcb-729e-4b92-ba27-f8b71421137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.mae_stft_real = nn.L1Loss()\n",
    "        self.mae_stft_imag = nn.L1Loss()\n",
    "        self.mae_inv_stft  = nn.L1Loss()\n",
    "    \n",
    "    def forward(self, pred_stft, target_stft,  pred_inv_stft, target_inv_stft):\n",
    "        loss_r = self.mae_stft_real(pred_stft.real, target_stft.real)\n",
    "        loss_i = self.mae_stft_imag(pred_stft.imag, target_stft.imag)\n",
    "        loss_t = self.mae_inv_stft(pred_inv_stft, target_inv_stft)\n",
    "        loss = loss_r + loss_i + loss_t\n",
    "        return loss\n",
    "\n",
    "def compute_usdr(pred, target, delta = 1e-7):\n",
    "    if pred.shape[0] < target.shape[0]:\n",
    "        padding = target.shape[0] - pred.shape[0]\n",
    "        pred = torch.nn.functional.pad(pred, (0, padding), \"constant\", 0)\n",
    "    num = torch.sum(torch.square(target))\n",
    "    den = torch.sum(torch.square(target - pred))\n",
    "    num += delta\n",
    "    den += delta\n",
    "    usdr = 10 * torch.log10(num / den)\n",
    "    return usdr.mean()    \n",
    "\n",
    "if visualize:\n",
    "    torch.set_default_device('cpu')\n",
    "    target_wf = load_audio(\"drums.wav\")\n",
    "    normal_target_wf, gain_factor, peak_gain_factor = normalize_waveform(target_wf)\n",
    "    target_wf_chunks, padding_length = split(normal_target_wf)\n",
    "    stft = to_spectogram()\n",
    "    inv_stft = from_spectogram()\n",
    "    print(\"target:\")\n",
    "    show_idp_audio(target_wf_chunks[1])\n",
    "    target_stft = stft(target_wf_chunks[1], visualize=True, title=\"Target\")\n",
    "    print(\"input:\")\n",
    "    show_idp_audio(sample_waveform_chunks[1])\n",
    "    sample_stft = stft(sample_waveform_chunks[1], visualize=True, title=\"Input\")\n",
    "\n",
    "    # x * mask = target_stft / x\n",
    "    # f(x) = stft - \n",
    "\n",
    "    mask_stft = target_stft / sample_stft\n",
    "    visualize_spectogram(inv_stft(mask_stft), mask_stft, title=\"Mask stft\")\n",
    "    print(\"Mask:\")\n",
    "    show_idp_audio(inv_stft(mask_stft))\n",
    "    target_y = sample_stft * mask_stft\n",
    "    print(\"Target_y: \")\n",
    "    show_idp_audio(inv_stft(target_y))\n",
    "    visualize_spectogram(inv_stft(target_y), target_y, title=\"Target_y stft calc\")\n",
    "\n",
    "\n",
    "    loss_fn = CustomLoss()\n",
    "    loss = loss_fn.forward(target_y, target_stft, inv_stft(target_y), inv_stft(target_stft))\n",
    "    print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a75952a2-8af0-465a-8db3-9259e55768cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d61bde73-fda7-4ef3-b5b0-f50b37af9daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_default_device('cuda')\n",
    "model = torch.load(\"./bsrnn-102-0.031533900648355484.pt\", map_location=\"cuda\")\n",
    "# b, (mix_stft, mask_stft) = next(enumerate(data_loader))\n",
    "# model.eval()\n",
    "# traced = torch.jit.trace(model, mix_stft.to('cuda'))\n",
    "\n",
    "\n",
    "\n",
    "# model = BSRNN()\n",
    "# torch.save(model, \"a.pt\")\n",
    "# print(model)\n",
    "# torch._dynamo.reset()\n",
    "#model = torch.compile(model, mode=\"reduce-overhead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a42ddce-5b0b-4085-98d2-8f34cb5a8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.onnx.export(model,               # model being run\n",
    "#                   mix_stft.to('cuda'),                         # model input (or a tuple for multiple inputs)\n",
    "#                   \"my_bsrnn.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "#                   export_params=True,        # store the trained parameter weights inside the model file\n",
    "#                   opset_version=10,          # the ONNX version to export the model to\n",
    "#                   do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "#                   input_names = ['input'],   # the model's input names\n",
    "#                   output_names = ['output'], # the model's output names\n",
    "#                   dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "#                                 'output' : {0 : 'batch_size'}})\n",
    "# torch.jit.save(traced, \"traced.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82d5ac67-449f-48ca-9dc8-831afd846429",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 11:51:20.802442: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-21 11:51:20.802487: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-21 11:51:20.803120: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-21 11:51:20.807526: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-21 11:51:21.326196: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up data loader\n",
      "model params: 698\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "import math\n",
    "import json\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "batch_size=95\n",
    "\n",
    "torch.set_default_device('cpu')\n",
    "\n",
    "class Dataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, path=\"/home/sahand/BandSplit-RNN/musdb18hq/train\"):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.files = [entry for entry in os.scandir(path) if entry.is_dir()]\n",
    "        random.shuffle(self.files)\n",
    "    \n",
    "    def iterator(self):\n",
    "        torch.set_default_device('cpu')\n",
    "        to_stft = to_spectogram()\n",
    "        for i in range(self.start_index, self.end_index):\n",
    "            d = self.files[i]\n",
    "            mixture = f\"{d.path}/mixture.wav\"\n",
    "            target = f\"{d.path}/drums.wav\"\n",
    "\n",
    "            normal_mix, _, _ = normalize_waveform(load_audio(mixture))\n",
    "            normal_target, _, _ = normalize_waveform(load_audio(target))\n",
    "\n",
    "            normal_mix, _ = split(normal_mix)\n",
    "            normal_target, _ = split(normal_target)\n",
    "\n",
    "            for mix, target in zip(normal_mix, normal_target):\n",
    "                mix_stft = to_stft(mix)\n",
    "                target_stft = to_stft(target)\n",
    "                # Accumulate STFTs in the batch\n",
    "                yield (mix_stft, target_stft)\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is not None:\n",
    "            total_files = len(self.files)\n",
    "            per_worker = int(math.ceil(total_files / float(worker_info.num_workers)))\n",
    "            self.start_index = worker_info.id * per_worker\n",
    "            self.end_index = min(self.start_index + per_worker, total_files)\n",
    "        else:\n",
    "            self.start_index = 0\n",
    "            self.end_index = len(self.files)\n",
    "\n",
    "        return iter(self.iterator())\n",
    "\n",
    "print(\"Setting up data loader\")\n",
    "data_loader = torch.utils.data.DataLoader(Dataset(), num_workers=3, \n",
    "                                          batch_size=batch_size, drop_last=True, \n",
    "                                          # pin_memory=True, pin_memory_device='cuda',\n",
    "                                          prefetch_factor=4,\n",
    "                                          persistent_workers=True)\n",
    "\n",
    "\n",
    "torch.set_default_device('cuda')\n",
    "print(f\"model params: {len(list(model.parameters()))}\")\n",
    "lr = 1e-5\n",
    "clip_grad_norm = 5\n",
    "optimizer = torch.optim.Adam(model.parameters() ,lr=lr)\n",
    "loss_fn = CustomLoss()\n",
    "\n",
    "inv_stft_gpu = from_spectogram()\n",
    "#name = \"logs/reversed-mask/2023-12-18 05:56:45.991714\"\n",
    "\n",
    "n_epochs = 500\n",
    "start_epoch = 103\n",
    "name = f'logs/reversed-mask/{datetime.datetime.now()}'\n",
    "writer = SummaryWriter(name)\n",
    "print(f\"logdir = {name}\")\n",
    "hparams = {\n",
    "    \"lr\": lr,\n",
    "    \"capacity\": len(list(model.parameters())),\n",
    "    \"clip_grad_norm\": clip_grad_norm,\n",
    "    # \"eps\": eps,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"sample_rate\": sample_rate,\n",
    "    \"chunk_size_in_seconds\": chunk_size_in_seconds,\n",
    "    \"chunk_size\": chunk_size,\n",
    "    \"spectogram/n_fft\": n_fft,\n",
    "    \"spectogram/win_length\": win_length,\n",
    "    \"spectogram/hop_length\": hop_length,\n",
    "    \"model/feature_dim\": num_blstm_layers,\n",
    "}\n",
    "writer.add_text(\"Hyper parameters\", f\"{json.dumps(hparams)}\")\n",
    "\n",
    "\n",
    "max_b = 239\n",
    "for e in range(n_epochs - start_epoch):\n",
    "    epoch = e + start_epoch\n",
    "    total_loss = 0\n",
    "    total_usdr = 0\n",
    "    total_n = 0\n",
    "    loss = None\n",
    "    start = time.time()\n",
    "    # with torch.profiler.profile(\n",
    "    #     schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
    "    #     on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs/prof'),\n",
    "    #     record_shapes=True,\n",
    "    #     profile_memory=True,\n",
    "    #     with_stack=True\n",
    "    # ) as prof:\n",
    "    \n",
    "    for b, (mix_stft, mask_stft) in enumerate(data_loader):\n",
    "        max_b = max(b, max_b)\n",
    "        x = b + max_b * epoch\n",
    "        # prof.step()\n",
    "        #print(mix_stft.shape)\n",
    "        mix_stft = mix_stft.to('cuda')\n",
    "        mask_stft = mask_stft.to('cuda')\n",
    "        b_total = max_b\n",
    "        #with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True, with_stack=True) as prof:\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            y_mask = model(mix_stft)\n",
    "\n",
    "        #print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=100))\n",
    "        # print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=2))\n",
    "        inv_mask_stft = inv_stft_gpu(mask_stft)\n",
    "        inv_y_mask = inv_stft_gpu(y_mask)\n",
    "        loss = loss_fn(y_mask, mask_stft, inv_y_mask, inv_mask_stft)\n",
    "        total_loss += loss\n",
    "        total_n += 1\n",
    "        usdr = compute_usdr(inv_y_mask, inv_mask_stft)\n",
    "        total_usdr += usdr\n",
    "        mv_avg = total_loss/total_n\n",
    "        print(f\"Epoch {epoch}/{n_epochs} {b}/{b_total}: loss={loss} avg loss={mv_avg} usdr={usdr}\", end='\\r\\033[K\\r')\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        writer.add_scalar('Loss/train', loss, x)\n",
    "        writer.add_scalar('usdr/train', usdr, x)\n",
    "        writer.add_scalar('mv avg loss/train', mv_avg, x)\n",
    "    # break\n",
    "    avg_loss = total_loss/total_n\n",
    "    avg_usdr = total_usdr/total_n\n",
    "    writer.add_scalar('Avg Loss/train', avg_loss, epoch)\n",
    "    writer.add_scalar('Avg usdr/train', avg_usdr, epoch)\n",
    "    print(f\"epoch #{epoch} finished in {(time.time() - start) / 60}m. loss={avg_loss} usdr={avg_usdr}\\n\")\n",
    "    if epoch % 2 == 0:\n",
    "        torch.save(model, f\"bsrnn-{epoch}-{loss}.pt\")\n",
    "        # try:\n",
    "        #     m = MSSBandSplitRNN()\n",
    "        #     m.bsrnn = m\n",
    "        #     mix_valid = load_audio(\"mixture.wav\")\n",
    "        #     y = m(mix_valid.cuda())\n",
    "        #     writer.add_audio(f'Audio/example for {epoch}', y, epoch, sample_rate=sample_rate)\n",
    "        # except BaseException as error:\n",
    "        #     print(f\"error while writing audio example -> {error}\")\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2438305-edc1-4315-9341-4ec8914a7c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logdir = logs/reversed-mask/2023-12-21 11:51:22.940533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sahand/mambaforge/envs/bsrnn/lib/python3.10/site-packages/torch/utils/_device.py:77: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392067780/work/aten/src/ATen/EmptyTensor.cpp:31.)\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #103 finished in 12.799456389745076m. loss=0.06061919406056404 usdr=0.416151136159896855614013672\n",
      "\n",
      "\u001b[Kch 104/500 220/239: loss=0.0819278135895729 avg loss=0.0605936124920845 usdr=0.626477301120758119417\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip_grad_norm)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# update weights\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     70\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss/train\u001b[39m\u001b[38;5;124m'\u001b[39m, loss, x)\n",
      "File \u001b[0;32m~/mambaforge/envs/bsrnn/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/bsrnn/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/mambaforge/envs/bsrnn/lib/python3.10/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/mambaforge/envs/bsrnn/lib/python3.10/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/bsrnn/lib/python3.10/site-packages/torch/optim/adam.py:551\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    549\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_addcdiv_(device_params, device_exp_avgs, exp_avg_sq_sqrt)\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 551\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps]\n\u001b[1;32m    552\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps]\n\u001b[1;32m    554\u001b[0m     step_size \u001b[38;5;241m=\u001b[39m _stack_if_compiling([(lr \u001b[38;5;241m/\u001b[39m bc) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bc \u001b[38;5;129;01min\u001b[39;00m bias_correction1])\n",
      "File \u001b[0;32m~/mambaforge/envs/bsrnn/lib/python3.10/site-packages/torch/optim/adam.py:551\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    549\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_addcdiv_(device_params, device_exp_avgs, exp_avg_sq_sqrt)\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 551\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps]\n\u001b[1;32m    552\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps]\n\u001b[1;32m    554\u001b[0m     step_size \u001b[38;5;241m=\u001b[39m _stack_if_compiling([(lr \u001b[38;5;241m/\u001b[39m bc) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bc \u001b[38;5;129;01min\u001b[39;00m bias_correction1])\n",
      "File \u001b[0;32m~/mambaforge/envs/bsrnn/lib/python3.10/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_get_value\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/bsrnn/lib/python3.10/site-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d915e87-04be-4118-a414-47921130a907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "628\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c34477a-78c7-42c5-9e8a-1e7af36bd23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device('cuda')\n",
    "m = MSSBandSplitRNN()\n",
    "model.eval()\n",
    "m.bsrnn = model\n",
    "mixture_wav = load_audio(\"mixture.wav\")\n",
    "target_wav = load_audio(\"drums.wav\")\n",
    "with torch.no_grad():\n",
    "    y = m(mixture_wav.cuda())\n",
    "show_idp_audio(mixture_wav)\n",
    "show_idp_audio(y.cpu().detach().numpy())\n",
    "show_idp_audio(target_wav)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31915b7b-21ef-46cb-81a1-e9220b2fe4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf8d3a9-fca6-4926-8443-3fdc73278204",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.set_default_device('cuda')\n",
    "import glob\n",
    "models = list(glob.glob(\"*.pt\"))\n",
    "models = sorted(models, key=lambda x: int(x.split(\"-\")[1]))\n",
    "mixture_wav = load_audio(\"mixture.wav\")\n",
    "target_wav = load_audio(\"drums.wav\")\n",
    "print(\"mixture:\")\n",
    "show_idp_audio(mixture_wav)\n",
    "print(\"target:\")\n",
    "show_idp_audio(target_wav)\n",
    "\n",
    "dw = None\n",
    "for i, model_path in enumerate(models):\n",
    "    if i % 5 != 0:\n",
    "        continue\n",
    "    model_bsrnn = torch.load(model_path, map_location=\"cuda\")\n",
    "    m = MSSBandSplitRNN()\n",
    "    m.bsrnn = model_bsrnn\n",
    "    w = torch.cat([t.view(-1) for t in list(m.parameters())])\n",
    "    if dw is None:\n",
    "        dw = nn.L1Loss()(torch.zeros_like(w), w)\n",
    "    else:\n",
    "        dw = nn.L1Loss()(dw, w)\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            y = m(mixture_wav.cuda())\n",
    "        usdr = compute_usdr(y, target_wav.cuda())\n",
    "        print(f\"{model_path} w={dw} usdr={usdr}: \")\n",
    "        \n",
    "        show_idp_audio(y.cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fef9b44-0768-4b62-b242-0db193296eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf54d3df-82a2-48d4-be81-170ceff1190a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bsrnn]",
   "language": "python",
   "name": "conda-env-bsrnn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
